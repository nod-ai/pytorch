// !!! This is a file automatically generated by hipify!!!
#include <ATen/zoom/ZoomContext.h>
#include <ATen/zoom/detail/DeviceThreadHandles.h>

namespace at::zoom {
namespace {

void createHIPsolverDnHandle(hipsolverDnHandle_t *handle) {
  TORCH_HIPSOLVER_CHECK(hipsolverDnCreate(handle));
}

void destroyHIPsolverDnHandle(hipsolverDnHandle_t handle) {
// this is because of something dumb in the ordering of
// destruction. Sometimes atexit, the cuda context (or something)
// would already be destroyed by the time this gets destroyed. It
// happens in fbcode setting. @colesbury and @soumith decided to not destroy
// the handle as a workaround.
//   - Comments of @soumith copied from cuDNN handle pool implementation
#ifdef NO_CUDNN_DESTROY_HANDLE
  (void)handle; // Suppress unused variable warning
#else
    hipsolverDnDestroy(handle);
#endif
}

using HIPSolverDnPoolType = DeviceThreadHandlePool<hipsolverDnHandle_t, createHIPsolverDnHandle, destroyHIPsolverDnHandle>;

} // namespace

hipsolverDnHandle_t getCurrentHIPSolverDnHandle() {
  c10::DeviceIndex device = 0;
  C10_ZOOM_CHECK(c10::zoom::GetDevice(&device));

  // Thread local PoolWindows are lazily-initialized
  // to avoid initialization issues that caused hangs on Windows.
  // See: https://github.com/pytorch/pytorch/pull/22405
  // This thread local unique_ptrs will be destroyed when the thread terminates,
  // releasing its reserved handles back to the pool.
  static auto pool = std::make_shared<HIPSolverDnPoolType>();
  thread_local std::unique_ptr<HIPSolverDnPoolType::PoolWindow> myPoolWindow(
      pool->newPoolWindow());

  auto handle = myPoolWindow->reserve(device);
  auto stream = c10::zoom::getCurrentZoomStream();
  TORCH_HIPSOLVER_CHECK(hipsolverDnSetStream(handle, stream));
  return handle;
}

} // namespace at::zoom
