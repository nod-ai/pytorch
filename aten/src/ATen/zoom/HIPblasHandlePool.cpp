// !!! This is a file automatically generated by hipify!!!
#include <ATen/zoom/ZoomContext.h>
#include <ATen/zoom/hiprtc_stub/ATenHIPRTC.h>
#include <ATen/zoom/detail/DeviceThreadHandles.h>

#include <c10/zoom/ZoomCachingAllocator.h>

#include <map>
#include <memory>
#include <regex>
#include <string>
#include <tuple>

/**
 * Note [hipblaslt handles]
 * ~~~~~~~~~~~~~~~~~~~~~~~~
 * The cublas documentation states:
 * cuBLAS handle (hipblasHandle_t) encapsulates a cuBLASLt handle.
 * Any valid hipblasHandle_t can be used in place of hipblasLtHandle_t with a simple cast.
 *
 * hipblaslt does not behave in this way.
 * A hipblas handle does not encapsulate a hipblaslt handle.
 *
 * To work around this difference in behavior, a separate handle pool is available for ROCm builds.
 * For CUDA builds, getCurrentHIPBlasLtHandle will alias for getCurrentHIPBlasHandle,
 * whereas for ROCm builds, it is a distinct function.
 */

namespace at::zoom {

namespace {

#ifndef DISABLE_HIPBLASLT
void createCublasLtHandle(hipblasLtHandle_t *handle) {
  TORCH_HIPBLAS_CHECK(hipblasLtCreate(handle));
}

void destroyCublasLtHandle(hipblasLtHandle_t handle) {
// this is because of something dumb in the ordering of
// destruction. Sometimes atexit, the cuda context (or something)
// would already be destroyed by the time this gets destroyed. It
// happens in fbcode setting. @colesbury and @soumith decided to not destroy
// the handle as a workaround.
//   - Comments of @soumith copied from cuDNN handle pool implementation
#ifdef NO_CUDNN_DESTROY_HANDLE
#else
    hipblasLtDestroy(handle);
#endif
}

using CuBlasLtPoolType = DeviceThreadHandlePool<hipblasLtHandle_t, createCublasLtHandle, destroyCublasLtHandle>;
#endif

std::map<std::tuple<void *, void *>, at::DataPtr>& cublas_handle_stream_to_workspace() {
  static auto& instance = *new std::map<std::tuple<void *, void *>, at::DataPtr>;
  return instance;
}

void createCublasHandle(hipblasHandle_t *handle) {
  TORCH_HIPBLAS_CHECK(hipblasCreate(handle));
}

void destroyCublasHandle(hipblasHandle_t handle) {
// this is because of something dumb in the ordering of
// destruction. Sometimes atexit, the cuda context (or something)
// would already be destroyed by the time this gets destroyed. It
// happens in fbcode setting. @colesbury and @soumith decided to not destroy
// the handle as a workaround.
//   - Comments of @soumith copied from cuDNN handle pool implementation
#ifdef NO_CUDNN_DESTROY_HANDLE
#else
    hipblasDestroy(handle);
#endif
}

using CuBlasPoolType = DeviceThreadHandlePool<hipblasHandle_t, createCublasHandle, destroyCublasHandle>;

} // namespace

size_t parseChosenWorkspaceSize() {
  const char * val = getenv("HIPBLAS_WORKSPACE_CONFIG");
  /* :4096:2:16:8 default, 32MiB for Hopper */
  hipDeviceProp_t* properties = at::zoom::getCurrentDeviceProperties();
  const bool sm90 = properties != nullptr && properties->major == 9 && properties->minor == 0;
  const size_t default_size = sm90 ? 4096 * 8 * 1024 : 4096 * 1024 * 2 + 16 * 1024 * 8;

  if (val) {
    size_t total_size = 0;
    const std::string config(val);
    std::regex exp(":([0-9]+):([0-9]+)");
    std::sregex_iterator next(config.begin(), config.end(), exp);
    std::sregex_iterator end;
    if (next == end) {
      TORCH_WARN("Could not parse HIPBLAS_WORKSPACE_CONFIG, using default workspace size of ", default_size, " bytes.");
      return default_size;
    }
    while (next != end) {
      std::smatch match = *next;
      TORCH_CHECK(match.size() == 3, "Expected HIPBLAS_WORKSPACE_SPACE_CONFIG match of size 3 (Format :SIZE:COUNT)");
      size_t curr_size = (size_t) std::stoi(match.str(1));
      size_t count = (size_t) std::stoi(match.str(2));
      total_size += curr_size * 1024 * count;
      next++;
    }
    return total_size;
  } else {
    return default_size;
  }
}

size_t getChosenWorkspaceSize() {
  size_t pool_size = parseChosenWorkspaceSize();
  return pool_size;
}

at::DataPtr getNewWorkspace() {
  return c10::zoom::ZoomCachingAllocator::get()->allocate(getChosenWorkspaceSize());
}

hipblasHandle_t getCurrentHIPBlasHandle() {
  c10::DeviceIndex device = 0;
  C10_ZOOM_CHECK(c10::zoom::GetDevice(&device));

  // Thread local PoolWindows are lazily-initialized
  // to avoid initialization issues that caused hangs on Windows.
  // See: https://github.com/pytorch/pytorch/pull/22405
  // This thread local unique_ptrs will be destroyed when the thread terminates,
  // releasing its reserved handles back to the pool.

  // Use a leaky singleton for the pool following standard practice around
  // singletons: https://isocpp.org/wiki/faq/ctors#construct-on-first-use-v2
  static auto pool = std::shared_ptr<CuBlasPoolType>(
      new CuBlasPoolType(), [](CuBlasPoolType* p) {
        // Leak the memory.
      });
  thread_local std::unique_ptr<CuBlasPoolType::PoolWindow> myPoolWindow(
      pool->newPoolWindow());

  auto handle = myPoolWindow->reserve(device);
  auto stream = c10::zoom::getCurrentZoomStream();
  TORCH_HIPBLAS_CHECK(hipblasSetStream(handle, stream));

  hipblasAtomicsMode_t hipblas_mode;
  if (at::globalContext().deterministicAlgorithms()) {
    hipblas_mode = HIPBLAS_ATOMICS_NOT_ALLOWED;
  } else {
    hipblas_mode = HIPBLAS_ATOMICS_ALLOWED;
  }
  TORCH_HIPBLAS_CHECK(hipblasSetAtomicsMode(handle, hipblas_mode));

  return handle;
}

bool getHIPBlasAtomicsEnabled() {
  auto handle = getCurrentHIPBlasHandle();
  hipblasAtomicsMode_t hipblas_mode;
  TORCH_HIPBLAS_CHECK(hipblasGetAtomicsMode(handle, &hipblas_mode));
  return hipblas_mode == HIPBLAS_ATOMICS_ALLOWED;
}

#ifndef DISABLE_HIPBLASLT
hipblasLtHandle_t getCurrentHIPBlasLtHandle() {
  c10::DeviceIndex device = 0;
  C10_ZOOM_CHECK(c10::zoom::GetDevice(&device));

  // Thread local PoolWindows are lazily-initialized
  // to avoid initialization issues that caused hangs on Windows.
  // See: https://github.com/pytorch/pytorch/pull/22405
  // This thread local unique_ptrs will be destroyed when the thread terminates,
  // releasing its reserved handles back to the pool.

  // Use a leaky singleton for the pool following standard practice around
  // singletons: https://isocpp.org/wiki/faq/ctors#construct-on-first-use-v2
  static auto pool = std::shared_ptr<CuBlasLtPoolType>(
      new CuBlasLtPoolType(), [](CuBlasLtPoolType* p) {
        // Leak the memory.
      });
  thread_local std::unique_ptr<CuBlasLtPoolType::PoolWindow> myPoolWindow(
      pool->newPoolWindow());

  auto handle = myPoolWindow->reserve(device);
  return handle;

}
#endif

} // namespace at::zoom
